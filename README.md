# üìù DCGAN for Handwritten Digit Generation

## 1. üìò About
This project will use Deep Convolutional Generative Adversarial Networks for the generation of images of handwritten digits, the main focus will be on the MNIST dataset. Generative Adversarial Networks (GANs) are a class of machine learning frameworks in generative models used for generating new data samples so that the generative model looks like the underlying data distribution. A GAN consists of two neural networks, a **Generator** and a **Discriminator**, which are simultaneously trained but through an adversarial process.

### What is a GAN?
A GAN is composed of two models:
- **Generator**: This model generates fake data resembling the real dataset.
- **Discriminator**: This model evaluates whether the data generated by the generator is real or fake. 

The two models are engaged in a game where the generator tries to produce increasingly realistic data to fool the discriminator, while the discriminator attempts to become better at distinguishing real data from fake data. This adversarial process drives both models to improve over time, ideally leading to the generator creating data indistinguishable from the real dataset.

## 2. üåü Impact
This project demonstrates the potential of DCGANs in image generation tasks. The ability to generate high-quality synthetic images can be applied in various fields, such as:
- **Data Augmentation**: Enriching training datasets with synthetic images to improve model generalization.
- **Unsupervised Learning**: Learning representations without labeled data.
- **Creative AI**: Applications in art and design, generating new styles or designs based on existing ones.

## 3. üîç Methodology

### Data Preparation
The MNIST dataset, which contains 60,000 images of handwritten digits, is loaded and normalized. Each image is scaled to have pixel values in the range \([-1, 1]\), which is crucial for stable GAN training.

```python
(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()
train_images = train_images.reshape(train_images.shape[0], IMG_WIDTH, IMG_HEIGHT, 1).astype('float32')
train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]
```

### Model Architecture

#### Generator
The generator starts with a dense layer that takes a random noise vector as input and transforms it through a series of upsampling operations, ultimately producing a 28x28 grayscale image.

**Layers and Their Functions:**

1. **Dense Layer**:
   - **Input**: 100-dimensional noise vector.
   - **Output**: 7x7x256 tensor.
   - **Purpose**: The dense layer transforms the noise vector into a 3D tensor, which can be further processed into an image.

2. **BatchNormalization**:
   - **Input**: Output of the Dense layer.
   - **Output**: Normalized 7x7x256 tensor.
   - **Purpose**: Batch normalization helps in stabilizing the training process by normalizing the output of the dense layer.

3. **LeakyReLU**:
   - **Input**: Normalized tensor.
   - **Output**: Rectified tensor with small negative slope.
   - **Purpose**: LeakyReLU introduces non-linearity, allowing the model to learn complex patterns.

4. **Reshape**:
   - **Input**: 1D tensor from the dense layer.
   - **Output**: 3D tensor (7x7x256).
   - **Purpose**: Reshapes the 1D output into a 3D format suitable for convolutional operations.

5. **Conv2DTranspose (Upsampling)**:
   - **Input**: 7x7x256 tensor.
   - **Output**: Upsampled tensor to higher resolution (14x14x128, then 28x28x64).
   - **Purpose**: This layer performs upsampling, increasing the spatial dimensions of the image while reducing the number of channels.

6. **Final Conv2DTranspose Layer**:
   - **Input**: 14x14x64 tensor.
   - **Output**: 28x28x1 image.
   - **Purpose**: The final transposed convolution layer outputs the final image with a single channel (grayscale) and pixel values in the range \([-1, 1]\).

```python
def make_generator():
    model = tf.keras.Sequential()
    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Reshape((7, 7, 256)))
    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))

    return model
```

#### Discriminator
The discriminator is a Convolutional Neural Network (CNN) that classifies images as real or fake. It reduces the spatial dimensions while increasing depth, allowing the network to learn hierarchical features for classification.

**Layers and Their Functions:**

1. **Conv2D**:
   - **Input**: 28x28x1 image (real or generated).
   - **Output**: 14x14x64 tensor.
   - **Purpose**: The convolutional layer extracts features from the image, reducing the spatial size while increasing the depth (number of filters).

2. **LeakyReLU**:
   - **Input**: 14x14x64 tensor.
   - **Output**: Rectified tensor.
   - **Purpose**: Introduces non-linearity to help the model learn complex decision boundaries.

3. **Dropout**:
   - **Input**: Rectified tensor.
   - **Output**: Tensor with some values randomly set to zero.
   - **Purpose**: Prevents overfitting by randomly setting a fraction of the input units to zero during training.

4. **Conv2D**:
   - **Input**: 14x14x64 tensor.
   - **Output**: 7x7x128 tensor.
   - **Purpose**: Further extracts features, reducing the image size and increasing depth.

5. **Flatten**:
   - **Input**: 7x7x128 tensor.
   - **Output**: 1D vector.
   - **Purpose**: Flattens the output to a 1D vector, preparing it for the final dense layer.

6. **Dense**:
   - **Input**: 1D vector.
   - **Output**: Single scalar value.
   - **Purpose**: Outputs the probability of the image being real or fake.

```python
def make_discriminator():
    model = tf.keras.Sequential()
    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))
    model.add(LeakyReLU())
    model.add(Dropout(0.3))

    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(LeakyReLU())
    model.add(Dropout(0.3))

    model.add(Flatten())
    model.add(Dense(1))

    return model
```

### Training Process
The generator and discriminator are trained together in an adversarial loop. The generator's goal is to create realistic images that can fool the discriminator, while the discriminator aims to accurately classify images as real or fake.

```python
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
```

## 4. üõ†Ô∏è Technologies Used
- **TensorFlow**: A powerful framework for building and training deep learning models.
- **Keras**: A high-level neural networks API, integrated with TensorFlow.
- **Matplotlib**: A plotting library used for visualizing the dataset and generated images.
- **Imageio**: A Python library for reading and writing image data, used to create animated GIFs.

## 5. üî¨ Visualizations and Findings

### Preview of Data Samples from the MNIST Dataset

![41d6dbd7-dc8e-4651-89a5-8941adf48ed0](https://github.com/user-attachments/assets/008d80a7-3a60-4ce6-98f2-82fa4987f024)

This visualization displays a grid of sample images from the MNIST dataset, showcasing the variety of handwritten digits used for training the GAN.

### Generated Image Before Training

![e2ea10b7-ed37-41f2-b2c5-c057890dc347](https://github.com/user-attachments/assets/79cbc429-0773-487c-aa26-bddbe0c9f6d3)

This image shows the output of the generator model before any training has occurred. The generated image is essentially noise, illustrating the model's starting point before it learns to create realistic images.

### Generated Images After 300 Epochs

![25ba9ce2-35f1-446f-9b7c-a93381e940c6](https://github.com/user-attachments/assets/4fe36f72-cec0-4e39-8c01-838478929f0f)

After training the GAN for 300 epochs, the generator produces images that closely resemble the handwritten digits in the MNIST dataset. This result highlights the effectiveness of the adversarial training process.

### Generator Improvement Across Epochs

![download](https://github.com/user-attachments/assets/f66dd2dd-00a9-4a57-8d9d-b11975d1c77a)

This animated GIF shows the progression of generated images as the GAN trains over 300 epochs. It provides a visual representation of how the generator improves, producing increasingly realistic digits as training progresses.

## 6. üöß Challenges and How I Overcame Them
### Challenge: Balancing the Generator and Discriminator
One of the primary challenges in training GANs is maintaining a balance between the generator and discriminator. If one model becomes too strong, it can overpower the other, leading to suboptimal training.

### Solution:
- **Careful Initialization**: Ensured that both models started with comparable initial performance by using appropriate weight initializations.
- **Learning Rate Tuning**: Adjusted the learning rates for both models to maintain a balance during training.
- **Regularization**: Used dropout in the discriminator to prevent overfitting and ensure generalization.

## 7. üèÅ Conclusion
This project shows that it is possible to generate realistic-looking digits from noise input with the use of a DCGAN through the adversarial training process. The generator passes through the adversarial training process, thus coming up with high-quality images hard to differentiate from the real data. This work is a stepping stone for more complex image generation tasks and gains insight into the design and training of GANs.

## 8. üíª How to Run on Your Machine

### Prerequisites
- Python 3.x
- TensorFlow 2.x
- Keras
- Matplotlib
- Imageio

### Steps to Run
1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-repo/dcgan-mnist.git
   cd dcgan-mnist
   ```

2. **Install dependencies**:
   ```bash
   pip install tensorflow keras matplotlib imageio
   ```

3. **Run the Jupyter Notebook**:
   ```bash
   jupyter notebook model.ipynb
   ```

4. **Training the Model**:
   - Open the notebook and run all cells to start training the DCGAN model.
   - The generated images will be saved in the `images/` directory.
   - The saved checkpoints will be saved in the `training_checkpoints/` directory.

5. **Create a GIF**:
   - After training, the notebook will generate a GIF showing the progress of image generation across epochs.
